{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Jek36AkxVf0",
        "outputId": "832ba160-fd85-4927-f668-d95dc5b8148c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
            "Fitting 5 folds for each of 16 candidates, totalling 80 fits\n",
            "✅ Done. Results in /content/artifacts\n",
            "✅ Done. Results in /content/artifacts\n"
          ]
        }
      ],
      "source": [
        "#!/usr/bin/env python3\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Heart Disease — Comprehensive ML Pipeline (Single Python Script)\n",
        "- Data loading (UCI Cleveland)\n",
        "- Preprocessing & EDA (saves figures)\n",
        "- PCA (plots cumulative explained variance)\n",
        "- Feature selection (RF importance, RFE, Chi²)\n",
        "- Supervised models (LogReg, Decision Tree, RandomForest, SVM) + metrics & ROC\n",
        "- Unsupervised (KMeans elbow, Hierarchical dendrogram sample)\n",
        "- Hyperparameter tuning (RF randomized, SVM grid)\n",
        "- Export best model (joblib)\n",
        "Tested on: Python 3.9+\n",
        "\"\"\"\n",
        "from pathlib import Path\n",
        "\n",
        "# إعدادات\n",
        "class Args:\n",
        "    data = None\n",
        "    outdir = \"artifacts\"\n",
        "    eda = True\n",
        "    pca = True\n",
        "    feat = True\n",
        "    supervised = True\n",
        "    unsupervised = True\n",
        "    tune = True\n",
        "    export = True\n",
        "    do_all = False\n",
        "\n",
        "main(Args())\n",
        "\n",
        "\n",
        "import argparse\n",
        "import io\n",
        "import os\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import RFE, chi2\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, roc_curve, classification_report, confusion_matrix)\n",
        "from sklearn.base import clone\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "\n",
        "from joblib import dump\n",
        "\n",
        "UCI_URL = \"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\"\n",
        "COLUMNS = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang',\n",
        "           'oldpeak','slope','ca','thal','num']\n",
        "\n",
        "# --------------------------------\n",
        "# Helper Functions\n",
        "# --------------------------------\n",
        "\n",
        "def ensure_dirs(outdir: Path):\n",
        "    (outdir / \"figures\").mkdir(parents=True, exist_ok=True)\n",
        "    (outdir / \"models\").mkdir(parents=True, exist_ok=True)\n",
        "    (outdir / \"results\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "def load_data(path: str = None) -> pd.DataFrame:\n",
        "    if path and Path(path).exists():\n",
        "        df = pd.read_csv(path)\n",
        "        if \"target\" not in df.columns and \"num\" in df.columns:\n",
        "            df[\"target\"] = (df[\"num\"] > 0).astype(int)\n",
        "            df = df.drop(columns=[\"num\"])\n",
        "        return df\n",
        "    df = pd.read_csv(UCI_URL, header=None, names=COLUMNS, na_values='?')\n",
        "    df['target'] = (df['num'] > 0).astype(int)\n",
        "    df.drop(columns=['num'], inplace=True)\n",
        "    return df\n",
        "\n",
        "def build_preprocessor(X: pd.DataFrame):\n",
        "    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_features = [c for c in X.columns if c not in numeric_features]\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler())\n",
        "    ])\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "    ])\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_transformer, numeric_features),\n",
        "            (\"cat\", categorical_transformer, categorical_features)\n",
        "        ]\n",
        "    )\n",
        "    return preprocessor, numeric_features, categorical_features\n",
        "\n",
        "def do_eda(df: pd.DataFrame, outdir: Path):\n",
        "    info_buf = io.StringIO()\n",
        "    df.info(buf=info_buf)\n",
        "    (outdir / \"results\" / \"data_info.txt\").write_text(info_buf.getvalue())\n",
        "    (outdir / \"results\" / \"missing_values.txt\").write_text(str(df.isna().sum()))\n",
        "\n",
        "    df.hist(bins=20, figsize=(14,10))\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outdir / \"figures\" / \"histograms.png\", dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "    corr = df.select_dtypes(include=[np.number]).corr()\n",
        "    plt.figure(figsize=(9,7))\n",
        "    sns.heatmap(corr, annot=False)\n",
        "    plt.title(\"Correlation Heatmap\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outdir / \"figures\" / \"correlation_heatmap.png\", dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def do_pca(preprocessor, X_train, outdir: Path):\n",
        "    pca_pipeline = Pipeline(steps=[(\"pre\", preprocessor),\n",
        "                                  (\"pca\", PCA(n_components=None, random_state=42))])\n",
        "    pca_pipeline.fit(X_train)\n",
        "    pca = pca_pipeline.named_steps[\"pca\"]\n",
        "    explained = pca.explained_variance_ratio_\n",
        "    cum = np.cumsum(explained)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, len(explained)+1), cum, marker=\"o\")\n",
        "    plt.xlabel(\"Number of components\")\n",
        "    plt.ylabel(\"Cumulative explained variance\")\n",
        "    plt.title(\"PCA — Cumulative Explained Variance\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outdir / \"figures\" / \"pca_cumulative_variance.png\", dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "    comps_95 = int((cum>=0.95).argmax()+1) if len(cum)>0 else 0\n",
        "    (outdir / \"results\" / \"pca_variance.txt\").write_text(\n",
        "        f\"Components to reach ≥95% variance: {comps_95}\\n\"\n",
        "    )\n",
        "\n",
        "def feature_names_after_pre(preprocessor, numeric_features, categorical_features):\n",
        "    oh = preprocessor.transformers_[1][1].named_steps[\"onehot\"] if categorical_features else None\n",
        "    cat_names = list(oh.get_feature_names_out(categorical_features)) if oh is not None else []\n",
        "    return list(numeric_features) + cat_names\n",
        "\n",
        "def feature_selection(preprocessor, X_train, y_train, numeric_features, categorical_features, outdir: Path):\n",
        "    rf = Pipeline(steps=[(\"pre\", preprocessor),\n",
        "                        (\"model\", RandomForestClassifier(n_estimators=300, random_state=42))])\n",
        "    rf.fit(X_train, y_train)\n",
        "    feat_names = feature_names_after_pre(rf.named_steps[\"pre\"], numeric_features, categorical_features)\n",
        "    importances = rf.named_steps[\"model\"].feature_importances_\n",
        "    fi = pd.DataFrame({\"feature\": feat_names, \"importance\": importances}).sort_values(\"importance\", ascending=False)\n",
        "    fi.to_csv(outdir / \"results\" / \"feature_importances.csv\", index=False)\n",
        "\n",
        "    fi.head(20).sort_values(\"importance\").plot.barh(x=\"feature\", y=\"importance\")\n",
        "    plt.title(\"Top 20 Feature Importances (RandomForest)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outdir / \"figures\" / \"rf_feature_importances.png\", dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "    pre_only = clone(preprocessor).fit(X_train, y_train)\n",
        "    Xtr = pre_only.transform(X_train)\n",
        "    est = LogisticRegression(max_iter=2000)\n",
        "    selector = RFE(estimator=est, n_features_to_select=min(15, Xtr.shape[1]), step=1)\n",
        "    selector.fit(Xtr, y_train)\n",
        "    rfe_features = np.array(feature_names_after_pre(pre_only, numeric_features, categorical_features))[selector.get_support()]\n",
        "    (outdir / \"results\" / \"rfe_features.txt\").write_text(\"\\n\".join(map(str, rfe_features.tolist())))\n",
        "\n",
        "    chi_pre = ColumnTransformer(transformers=[\n",
        "        (\"num\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "                          (\"scale\", MinMaxScaler())]), numeric_features),\n",
        "        (\"cat\", Pipeline([(\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "                          (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))]), categorical_features)\n",
        "    ])\n",
        "    Xtr_chi = chi_pre.fit_transform(X_train)\n",
        "    chi_vals, chi_p = chi2(Xtr_chi, y_train)\n",
        "    chi_df = pd.DataFrame({\"feature\": feature_names_after_pre(chi_pre, numeric_features, categorical_features),\n",
        "                           \"chi2\": chi_vals, \"p_value\": chi_p}).sort_values(\"chi2\", ascending=False)\n",
        "    chi_df.to_csv(outdir / \"results\" / \"chi2_features.csv\", index=False)\n",
        "\n",
        "def train_supervised(preprocessor, X_train, X_test, y_train, y_test, outdir: Path):\n",
        "    models = {\n",
        "        \"LogReg\": LogisticRegression(max_iter=2000),\n",
        "        \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
        "        \"RandomForest\": RandomForestClassifier(n_estimators=300, random_state=42),\n",
        "        \"SVM_RBF\": SVC(kernel=\"rbf\", probability=True, random_state=42)\n",
        "    }\n",
        "\n",
        "    rows = {}\n",
        "    roc_curves = {}\n",
        "\n",
        "    for name, clf in models.items():\n",
        "        pipe = Pipeline([(\"pre\", preprocessor), (\"clf\", clf)])\n",
        "        pipe.fit(X_train, y_train)\n",
        "        y_pred = pipe.predict(X_test)\n",
        "\n",
        "        rows[name] = {\n",
        "            \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "            \"precision\": precision_score(y_test, y_pred),\n",
        "            \"recall\": recall_score(y_test, y_pred),\n",
        "            \"f1\": f1_score(y_test, y_pred)\n",
        "        }\n",
        "        if hasattr(pipe.named_steps[\"clf\"], \"predict_proba\"):\n",
        "            y_prob = pipe.predict_proba(X_test)[:, 1]\n",
        "            rows[name][\"roc_auc\"] = roc_auc_score(y_test, y_prob)\n",
        "            fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
        "            roc_curves[name] = (fpr, tpr)\n",
        "\n",
        "        cr = classification_report(y_test, y_pred, digits=3)\n",
        "        (outdir / \"results\" / f\"classification_report_{name}.txt\").write_text(cr)\n",
        "\n",
        "    pd.DataFrame(rows).T.to_csv(outdir / \"results\" / \"supervised_summary.csv\")\n",
        "\n",
        "    plt.figure()\n",
        "    for name, (fpr, tpr) in roc_curves.items():\n",
        "        plt.plot(fpr, tpr, label=name)\n",
        "    plt.plot([0,1],[0,1],'--')\n",
        "    plt.xlabel(\"FPR\"); plt.ylabel(\"TPR\")\n",
        "    plt.title(\"ROC Curves\")\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outdir / \"figures\" / \"roc_curves.png\", dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def do_unsupervised(preprocessor, X, y, outdir: Path):\n",
        "    pre_all = Pipeline([(\"pre\", preprocessor)]).fit(X)\n",
        "    X_all = pre_all.transform(X)\n",
        "\n",
        "    inertias, Ks = [], list(range(1, 10))\n",
        "    for k in Ks:\n",
        "        km = KMeans(n_clusters=k, n_init=10, random_state=42)\n",
        "        km.fit(X_all)\n",
        "        inertias.append(km.inertia_)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(Ks, inertias, marker=\"o\")\n",
        "    plt.xlabel(\"k\"); plt.ylabel(\"Inertia\")\n",
        "    plt.title(\"KMeans — Elbow\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outdir / \"figures\" / \"kmeans_elbow.png\", dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "    idx = np.random.RandomState(42).choice(X_all.shape[0], size=min(100, X_all.shape[0]), replace=False)\n",
        "    Z = linkage(X_all[idx], method=\"ward\")\n",
        "    plt.figure(figsize=(10,5))\n",
        "    dendrogram(Z, truncate_mode=\"level\", p=5)\n",
        "    plt.title(\"Hierarchical Clustering — Dendrogram (sample)\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(outdir / \"figures\" / \"hierarchical_dendrogram.png\", dpi=160)\n",
        "    plt.close()\n",
        "\n",
        "def tune_models(preprocessor, X_train, y_train, X_test, y_test, outdir: Path):\n",
        "    from scipy.stats import randint\n",
        "\n",
        "    rf_pipe = Pipeline([(\"pre\", preprocessor),\n",
        "                        (\"clf\", RandomForestClassifier(random_state=42))])\n",
        "\n",
        "    rf_params = {\n",
        "        \"clf__n_estimators\": randint(100, 600),\n",
        "        \"clf__max_depth\": randint(2, 20),\n",
        "        \"clf__min_samples_split\": randint(2, 20),\n",
        "        \"clf__min_samples_leaf\": randint(1, 10)\n",
        "    }\n",
        "\n",
        "    rf_search = RandomizedSearchCV(rf_pipe, rf_params, n_iter=25, scoring=\"f1\",\n",
        "                                   cv=5, random_state=42, n_jobs=-1, verbose=1)\n",
        "    rf_search.fit(X_train, y_train)\n",
        "    rf_best = rf_search.best_estimator_\n",
        "\n",
        "    svm_pipe = Pipeline([(\"pre\", preprocessor),\n",
        "                         (\"clf\", SVC(kernel=\"rbf\", probability=True, random_state=42))])\n",
        "\n",
        "    svm_grid = {\n",
        "        \"clf__C\": [0.1, 1, 10, 100],\n",
        "        \"clf__gamma\": [\"scale\", 0.1, 0.01, 0.001]\n",
        "    }\n",
        "\n",
        "    svm_search = GridSearchCV(svm_pipe, svm_grid, scoring=\"f1\", cv=5, n_jobs=-1, verbose=1)\n",
        "    svm_search.fit(X_train, y_train)\n",
        "    svm_best = svm_search.best_estimator_\n",
        "\n",
        "    return rf_best, svm_best\n",
        "\n",
        "def export_best_model(rf_best, svm_best, X_test, y_test, outdir: Path):\n",
        "    rf_f1 = f1_score(y_test, rf_best.predict(X_test))\n",
        "    svm_f1 = f1_score(y_test, svm_best.predict(X_test))\n",
        "    best_model = rf_best if rf_f1 >= svm_f1 else svm_best\n",
        "    dump(best_model, outdir / \"models\" / \"heart_best_model.joblib\")\n",
        "    (outdir / \"results\" / \"best_model.txt\").write_text(f\"Selected model F1: {max(rf_f1, svm_f1)}\")\n",
        "\n",
        "# --------------------------------\n",
        "# Main Execution\n",
        "# --------------------------------\n",
        "\n",
        "def main(args):\n",
        "    outdir = Path(args.outdir)\n",
        "    ensure_dirs(outdir)\n",
        "    df = load_data(args.data)\n",
        "    if args.eda:\n",
        "        do_eda(df, outdir)\n",
        "    X = df.drop(columns=[\"target\"])\n",
        "    y = df[\"target\"]\n",
        "    preprocessor, num_cols, cat_cols = build_preprocessor(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "    if args.pca:\n",
        "        do_pca(preprocessor, X_train, outdir)\n",
        "    if args.feat:\n",
        "        feature_selection(preprocessor, X_train, y_train, num_cols, cat_cols, outdir)\n",
        "    if args.supervised:\n",
        "        train_supervised(preprocessor, X_train, X_test, y_train, y_test, outdir)\n",
        "    if args.unsupervised:\n",
        "        do_unsupervised(preprocessor, X, y, outdir)\n",
        "    if args.tune:\n",
        "        rf_best, svm_best = tune_models(preprocessor, X_train, y_train, X_test, y_test, outdir)\n",
        "        if args.export:\n",
        "            export_best_model(rf_best, svm_best, X_test, y_test, outdir)\n",
        "    print(f\"✅ Done. Results in {outdir.resolve()}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--data\", type=str, default=None)\n",
        "    parser.add_argument(\"--outdir\", type=str, default=\"artifacts\")\n",
        "    parser.add_argument(\"--eda\", action=\"store_true\")\n",
        "    parser.add_argument(\"--pca\", action=\"store_true\")\n",
        "    parser.add_argument(\"--feat\", action=\"store_true\")\n",
        "    parser.add_argument(\"--supervised\", action=\"store_true\")\n",
        "    parser.add_argument(\"--unsupervised\", action=\"store_true\")\n",
        "    parser.add_argument(\"--tune\", action=\"store_true\")\n",
        "    parser.add_argument(\"--export\", action=\"store_true\")\n",
        "    parser.add_argument(\"--do_all\", action=\"store_true\")\n",
        "    args = parser.parse_args()\n",
        "    if args.do_all:\n",
        "        args.eda = args.pca = args.feat = args.supervised = args.unsupervised = args.tune = True\n",
        "        args.export = True\n",
        "    main(args)\n"
      ]
    }
  ]
}